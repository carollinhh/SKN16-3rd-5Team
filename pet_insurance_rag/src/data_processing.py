# =========================================
# ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ
# =========================================

import os
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
from collections import defaultdict

# LangChain ì„í¬íŠ¸
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.schema import Document
from langchain_community.retrievers import BM25Retriever

# ì„¤ì • ì„í¬íŠ¸
try:
    from config.settings import *
except ImportError:
    # ê¸°ë³¸ ì„¤ì •ê°’
    VECTOR_BACKEND = "faiss"
    OPENAI_EMB_MODEL = "text-embedding-ada-002"
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200

class PetInsuranceGuard:
    """í«ë³´í—˜ ì•½ê´€ ì „ì²˜ë¦¬ ë° ì •ì œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ë¶ˆìš©ì–´ ë° í•„í„°ë§ ê·œì¹™
        self.stop_words = {
            'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ë“¤', 'ìˆ˜', 'ìˆ', 'ì—†', 'í•˜', 'ë˜', 'ëœ', 'ë ', 'í•¨', 
            'ë°', 'ë“±', 'ì˜', 'ë¥¼', 'ì—', 'ë¡œ', 'ê³¼', 'ì™€', 'ì€', 'ëŠ”', 'ì´', 'ê°€',
            'ë³´í—˜', 'ê°€ì…', 'ê³„ì•½', 'ì•½ê´€', 'ì¡°í•­', 'ê·œì •', 'ë‚´ìš©', 'ì‚¬í•­', 'ê²½ìš°',
            'ë•Œë¬¸', 'ë”°ë¼', 'ê´€ë ¨', 'ëŒ€í•œ', 'ìœ„í•œ', 'í†µí•´', 'ì´ìš©', 'ì œê³µ', 'ì²˜ë¦¬'
        }
        
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text).strip()
        text = text.replace('\n', ' ').replace('\r', ' ')
        text = ' '.join(text.split())
        
        return text
    
    def filter_relevant_content(self, text: str) -> bool:
        """í«ë³´í—˜ ê´€ë ¨ ë‚´ìš©ë§Œ í•„í„°ë§"""
        if not text or len(text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
            return False
            
        # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ë³´í—˜ ê´€ë ¨ ë‚´ìš©ì„ í¬í•¨
        insurance_keywords = [
            'ë³´í—˜', 'ë³´ì¥', 'ì•½ê´€', 'ê³„ì•½', 'ê°€ì…', 'ì²­êµ¬', 'ì§€ê¸‰', 'ë³´ìƒ',
            'ì¹˜ë£Œ', 'ì§„ë£Œ', 'ì˜ë£Œ', 'ë³‘ì›', 'ìˆ˜ìˆ ', 'ì…ì›', 'í†µì›',
            'ì§ˆë³‘', 'ìƒí•´', 'ì‚¬ê³ ', 'ë¶€ìƒ', 'ì˜ˆë°©', 'ê±´ê°•',
            'ë°˜ë ¤ë™ë¬¼', 'í«', 'ì• ì™„ë™ë¬¼', 'ê°•ì•„ì§€', 'ê³ ì–‘ì´', 'ë™ë¬¼'
        ]
        
        text_lower = text.lower()
        
        # ë³´í—˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í¬í•¨
        if any(keyword in text_lower for keyword in insurance_keywords):
            return True
        
        # ìˆ«ìë‚˜ ê¸ˆì•¡ì´ í¬í•¨ëœ ê²½ìš°ë„ í¬í•¨ (ë³´í—˜ê¸ˆ ê´€ë ¨)
        if any(char.isdigit() for char in text) and ('ì›' in text or 'ë§Œ' in text or '%' in text):
            return True
        
        return False
    
    def extract_structured_info(self, text: str) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        info = {
            'has_coverage_info': False,
            'has_exclusion_info': False,
            'has_procedure_info': False,
            'coverage_percentage': None,
            'max_amount': None
        }
        
        text_lower = text.lower()
        
        coverage_keywords = ['ë³´ì¥', 'ì§€ê¸‰', 'ë³´ìƒ', 'ê¸‰ì—¬']
        if any(keyword in text_lower for keyword in coverage_keywords):
            info['has_coverage_info'] = True
        
        exclusion_keywords = ['ë©´ì±…', 'ì œì™¸', 'ë³´ì¥í•˜ì§€', 'ì§€ê¸‰í•˜ì§€']
        if any(keyword in text_lower for keyword in exclusion_keywords):
            info['has_exclusion_info'] = True
        
        procedure_keywords = ['ì‹ ì²­', 'ì ‘ìˆ˜', 'ì œì¶œ', 'ì²­êµ¬']
        if any(keyword in text_lower for keyword in procedure_keywords):
            info['has_procedure_info'] = True
        
        return info

class LangChainEmbeddingsManager:
    """ì„ë² ë”© ë° ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or OPENAI_EMB_MODEL
        self.embeddings = None
        self._setup_embeddings()
    
    def _setup_embeddings(self):
        """ìºì‹œ ê¸°ë°˜ ì„ë² ë”© ì„¤ì •"""
        # API í‚¤ í™•ì¸
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸
        base_embeddings = OpenAIEmbeddings(
            model=self.model_name,
            openai_api_key=api_key
        )
        
        # ìºì‹œ ì €ì¥ì†Œ ì„¤ì •
        cache_dir = "./embeddings_cache"
        os.makedirs(cache_dir, exist_ok=True)
        store = LocalFileStore(cache_dir)
        
        # ìºì‹œ ê¸°ë°˜ ì„ë² ë”©
        self.embeddings = CacheBackedEmbeddings.from_bytes_store(
            base_embeddings, 
            store,
            namespace=f"openai_{self.model_name}"
        )
        
        print(f"âœ… ìºì‹œ ê¸°ë°˜ ì„ë² ë”© ì„¤ì • ì™„ë£Œ: {self.model_name}")
    
    def get_embeddings(self):
        """ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        return self.embeddings

class CompanyVectorStoreManager:
    """íšŒì‚¬ë³„ ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, embeddings_manager: LangChainEmbeddingsManager):
        self.embeddings = embeddings_manager.get_embeddings()
        self.guard = PetInsuranceGuard()
        self.company_vector_stores = {}
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ".", "!", "?", " ", ""]
        )
        
        # CSV íŒŒì¼ ë§¤í•‘
        from config.settings import CSV_FILES, DATA_DIR
        self.csv_files = {}
        for company, filename in CSV_FILES.items():
            file_path = DATA_DIR / filename
            self.csv_files[company] = str(file_path)

        
    def load_company_data(self, company: str, csv_file: str) -> List[Document]:
        """íšŒì‚¬ë³„ CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"ğŸ“Š {company} ë°ì´í„° ë¡œë”© ì¤‘...")
        
        if not os.path.exists(csv_file):
            print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
            return []
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(csv_file, encoding='cp949')
            except Exception as e:
                print(f"âŒ {csv_file} ì½ê¸° ì‹¤íŒ¨: {e}")
                return []
        
        documents = []
        processed_count = 0
        
        for idx, row in df.iterrows():
            # CSV êµ¬ì¡°ì— ë§ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = None
            
            # ì‹¤ì œ CSV ì»¬ëŸ¼ì— ë§ê²Œ ìˆ˜ì •
            if 'chunk_text' in df.columns and pd.notna(row['chunk_text']):
                text = str(row['chunk_text'])
            elif 'text' in df.columns and pd.notna(row['text']):
                text = str(row['text'])
            elif 'content' in df.columns and pd.notna(row['content']):
                text = str(row['content'])
            else:
                # ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì‹œë„
                text_columns = ['claim_sentence', 'preview', 'data', 'document']
                for col in text_columns:
                    if col in df.columns and pd.notna(row[col]):
                        text = str(row[col])
                        break
            
            if not text or len(text.strip()) < 10:
                continue
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_text = self.guard.clean_text(text)
            
            # ê´€ë ¨ì„± í•„í„°ë§
            if not self.guard.filter_relevant_content(cleaned_text):
                continue
            
            # êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
            structured_info = self.guard.extract_structured_info(cleaned_text)
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                'company': company,
                'source': os.path.basename(csv_file),
                'chunk_id': row.get('chunk_id', idx),
                'row_index': idx,
                **structured_info
            }
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            for col in ['page', 'section_name', 'subsection_name', 'subject', 'procedure']:
                if col in df.columns and pd.notna(row[col]):
                    metadata[col] = row[col]
            
            # Document ìƒì„±
            doc = Document(
                page_content=cleaned_text,
                metadata=metadata
            )
            
            documents.append(doc)
            processed_count += 1
        
        print(f"âœ… {company}: {processed_count}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
        return documents
    
    def create_company_vector_store(self, company: str, documents: List[Document]) -> FAISS:
        """íšŒì‚¬ë³„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        print(f"ğŸ” {company} ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        if not documents:
            raise ValueError(f"{company}ì— ëŒ€í•œ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        all_chunks = []
        for doc in documents:
            chunks = self.text_splitter.split_documents([doc])
            all_chunks.extend(chunks)
        
        print(f"ğŸ“‘ {company}: {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        batch_size = 50
        batches = [all_chunks[i:i + batch_size] for i in range(0, len(all_chunks), batch_size)]
        
        vector_store = None
        
        try:
            first_batch = batches[0]
            vector_store = FAISS.from_documents(first_batch, self.embeddings)
            print(f"   âœ… ë°°ì¹˜ 1/{len(batches)} ì™„ë£Œ ({len(first_batch)}ê°œ ë¬¸ì„œ)")
        except Exception as e:
            print(f"   âŒ ì²« ë²ˆì§¸ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
            raise
        
        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë“¤ ì¶”ê°€
        for i, batch in enumerate(batches[1:], 2):
            try:
                batch_vector_store = FAISS.from_documents(batch, self.embeddings)
                vector_store.merge_from(batch_vector_store)
                print(f"   âœ… ë°°ì¹˜ {i}/{len(batches)} ì™„ë£Œ ({len(batch)}ê°œ ë¬¸ì„œ)")
            except Exception as e:
                print(f"   âš ï¸ ë°°ì¹˜ {i} ì‹¤íŒ¨, ê±´ë„ˆëœ€: {e}")
                continue
        
        print(f"ğŸ‰ {company} ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
        return vector_store
    
    def load_all_companies(self) -> Dict[str, FAISS]:
        """ëª¨ë“  íšŒì‚¬ ë°ì´í„° ë¡œë“œ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        print("ğŸš€ ì „ì²´ íšŒì‚¬ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        for company, csv_file in self.csv_files.items():
            if not os.path.exists(csv_file):
                print(f"âš ï¸ {csv_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {company} ê±´ë„ˆëœ€.")
                continue
            
            try:
                # ë°ì´í„° ë¡œë“œ
                documents = self.load_company_data(company, csv_file)
                
                if documents:
                    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                    vector_store = self.create_company_vector_store(company, documents)
                    self.company_vector_stores[company] = vector_store
                else:
                    print(f"âš ï¸ {company}: ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            except Exception as e:
                print(f"âŒ {company} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        total_companies = len(self.company_vector_stores)
        total_docs = sum(len(vs.docstore._dict) for vs in self.company_vector_stores.values())
        
        print(f"ğŸ‰ ì „ì²´ ë¡œë”© ì™„ë£Œ!")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ íšŒì‚¬: {total_companies}ê°œ")
        print(f"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
        
        return self.company_vector_stores

class CompanyFilteredRetriever:
    """íšŒì‚¬ë³„ í•„í„°ë§ëœ ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self, company_vector_stores: Dict[str, FAISS]):
        self.company_vector_stores = company_vector_stores
        self.bm25_retrievers = {}
        self._setup_bm25_retrievers()
    
    def _setup_bm25_retrievers(self):
        """íšŒì‚¬ë³„ BM25 ê²€ìƒ‰ê¸° ì„¤ì •"""
        print("ğŸ” íšŒì‚¬ë³„ BM25 ê²€ìƒ‰ê¸° ì„¤ì • ì¤‘...")
        
        for company, vector_store in self.company_vector_stores.items():
            try:
                docs = list(vector_store.docstore._dict.values())
                
                if docs:
                    bm25_retriever = BM25Retriever.from_documents(docs)
                    bm25_retriever.k = 5
                    self.bm25_retrievers[company] = bm25_retriever
                    print(f"   âœ… {company} BM25 ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ")
                else:
                    print(f"   âš ï¸ {company}: ë¬¸ì„œê°€ ì—†ì–´ BM25 ê²€ìƒ‰ê¸° ìƒì„± ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"   âŒ {company} BM25 ê²€ìƒ‰ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
    
    def get_retriever(self, company: str = None, k: int = 5):
        """íšŒì‚¬ë³„ ê²€ìƒ‰ê¸° ë°˜í™˜"""
        if company and company in self.company_vector_stores:
            vector_store = self.company_vector_stores[company]
            try:
                return vector_store.as_retriever(search_kwargs={"k": k})
            except Exception as e:
                print(f"âš ï¸ {company} ê²€ìƒ‰ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
                return vector_store.as_retriever(search_kwargs={"k": k})
        else:
            if self.company_vector_stores:
                first_company = list(self.company_vector_stores.keys())[0]
                return self.company_vector_stores[first_company].as_retriever(search_kwargs={"k": k})
            else:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

def format_docs(docs: List[Document]) -> str:
    """ë¬¸ì„œë“¤ì„ ì•ˆì „í•˜ê²Œ í¬ë§·íŒ…"""
    if not docs:
        return ""
    
    formatted_docs = []
    for doc in docs:
        if doc is None:
            continue
            
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        
        if hasattr(doc, 'metadata') and doc.metadata:
            meta = doc.metadata
            company = meta.get('company', 'Unknown')
            source = meta.get('source', 'Unknown')
            formatted_docs.append(f"[{company}] {content}")
        else:
            formatted_docs.append(content)
    
    return "\n\n".join(formatted_docs)

def initialize_data_processing():
    """ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ ì´ˆê¸°í™”"""
    print("ğŸ”§ ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘...")
    
    # API í‚¤ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        # ë°±ì—…: openaikey.txt íŒŒì¼ì—ì„œ ì½ê¸° ì‹œë„
        try:
            with open('openaikey.txt', 'r') as f:
                api_key = f.read().strip()
            os.environ['OPENAI_API_KEY'] = api_key
            print("âœ… OpenAI API í‚¤ë¥¼ ë°±ì—… íŒŒì¼(openaikey.txt)ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except FileNotFoundError:
            raise ValueError(
                "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:\n"
                "1. .env íŒŒì¼ì— OPENAI_API_KEY=your-key ì¶”ê°€\n"
                "2. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì„¤ì •\n"
                "3. openaikey.txt íŒŒì¼ ìƒì„±"
            )
    else:
        print("âœ… OpenAI API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    # ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
    embeddings_manager = LangChainEmbeddingsManager()
    
    # ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    vector_manager = CompanyVectorStoreManager(embeddings_manager)
    
    # ëª¨ë“  íšŒì‚¬ ë°ì´í„° ë¡œë“œ
    company_vector_stores = vector_manager.load_all_companies()
    
    # ê²€ìƒ‰ê¸° ì„¤ì •
    filtered_retriever = CompanyFilteredRetriever(company_vector_stores)
    
    # íšŒì‚¬ë³„ ê°œë³„ ê²€ìƒ‰ê¸° ìƒì„±
    filtered_retrievers = {}
    for company in company_vector_stores.keys():
        filtered_retrievers[company] = filtered_retriever.get_retriever(company)
    
    print("âœ… ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    return {
        'embeddings': embeddings_manager.get_embeddings(),
        'company_vector_stores': company_vector_stores,
        'filtered_retrievers': filtered_retrievers,
        'embeddings_manager': embeddings_manager,
        'vector_manager': vector_manager,
        'filtered_retriever': filtered_retriever
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = initialize_data_processing()
    print(f"ğŸ“Š ì²˜ë¦¬ëœ íšŒì‚¬ ìˆ˜: {len(result['company_vector_stores'])}")
    print(f"ğŸ” ê²€ìƒ‰ê¸° ìˆ˜: {len(result['filtered_retrievers'])}")