"""
í«ë³´í—˜ RAG ì‹œìŠ¤í…œ í‰ê°€ ëª¨ë“ˆ
- ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë¶„ì„
- A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- GPT ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
"""

import sqlite3
import json
import statistics
import threading
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from collections import defaultdict, Counter
from pathlib import Path


class UserFeedbackEvaluator:
    """ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(self, db_path: str = "user_feedback.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
        CREATE TABLE IF NOT EXISTS feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT NOT NULL,
            query TEXT NOT NULL,
            company TEXT NOT NULL,
            answer TEXT NOT NULL,
            rating INTEGER NOT NULL,
            feedback_text TEXT,
            sources TEXT
        )
        ''')

        conn.commit()
        conn.close()

    def collect_feedback(self, query: str, company: str, answer: str, sources: list,
                        rating: int, feedback_text: str = ""):
        """í”¼ë“œë°± ìˆ˜ì§‘"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìŠ¤í‚¤ë§ˆë¥¼ ì—…ë°ì´íŠ¸
        cursor.execute("PRAGMA table_info(feedback)")
        columns = [row[1] for row in cursor.fetchall()]
        
        if 'query' not in columns:
            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±
            cursor.execute("DROP TABLE IF EXISTS feedback")
            self.init_database()

        sources_str = json.dumps(sources, ensure_ascii=False) if sources else ""
        timestamp = datetime.now().isoformat()

        cursor.execute('''
        INSERT INTO feedback (timestamp, query, company, answer, rating, feedback_text, sources)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (timestamp, query, company, answer, rating, feedback_text, sources_str))

        conn.commit()
        conn.close()

        print(f"âœ… í”¼ë“œë°± ì €ì¥ ì™„ë£Œ: {company} - í‰ì  {rating}/5")

    def analyze_feedback(self) -> Dict[str, Any]:
        """í”¼ë“œë°± ë¶„ì„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('SELECT * FROM feedback')
        all_feedback = cursor.fetchall()

        if not all_feedback:
            return {"error": "ìˆ˜ì§‘ëœ í”¼ë“œë°±ì´ ì—†ìŠµë‹ˆë‹¤."}

        # ì»¬ëŸ¼ ì´ë¦„ ë§¤í•‘
        columns = [desc[0] for desc in cursor.description]
        feedback_data = [dict(zip(columns, row)) for row in all_feedback]

        conn.close()

        # ë¶„ì„ ê²°ê³¼
        ratings = [f['rating'] for f in feedback_data]
        companies = [f['company'] for f in feedback_data]

        analysis = {
            'total_feedback': len(feedback_data),
            'average_rating': statistics.mean(ratings),
            'rating_distribution': Counter(ratings),
            'company_feedback_count': Counter(companies),
            'satisfaction_rate': len([r for r in ratings if r >= 4]) / len(ratings),
            'recent_feedback': feedback_data[-5:] if len(feedback_data) >= 5 else feedback_data
        }

        # íšŒì‚¬ë³„ í‰ê·  í‰ì 
        company_ratings = defaultdict(list)
        for f in feedback_data:
            company_ratings[f['company']].append(f['rating'])

        analysis['company_satisfaction'] = {
            company: statistics.mean(ratings)
            for company, ratings in company_ratings.items()
        }

        return analysis

    def generate_improvement_suggestions(self) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        analysis = self.analyze_feedback()

        if "error" in analysis:
            return ["í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ì–´ ê°œì„  ì œì•ˆì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

        suggestions = []

        # í‰ê·  í‰ì  ê¸°ë°˜ ì œì•ˆ
        avg_rating = analysis['average_rating']
        if avg_rating < 3.0:
            suggestions.append("ì „ë°˜ì ì¸ ë‹µë³€ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ì™„ì„±ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”.")
        elif avg_rating < 3.5:
            suggestions.append("ë‹µë³€ í’ˆì§ˆì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µì„ ê³ ë ¤í•´ì£¼ì„¸ìš”.")

        # íšŒì‚¬ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
        company_satisfaction = analysis.get('company_satisfaction', {})
        if company_satisfaction:
            best_company = max(company_satisfaction.items(), key=lambda x: x[1])
            worst_company = min(company_satisfaction.items(), key=lambda x: x[1])

            if best_company[1] - worst_company[1] > 1.0:
                suggestions.append(f"{worst_company[0]}ì˜ ë‹µë³€ í’ˆì§ˆì„ {best_company[0]} ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # í‰ì  ë¶„í¬ ë¶„ì„
        rating_dist = analysis['rating_distribution']
        low_ratings = rating_dist.get(1, 0) + rating_dist.get(2, 0)
        total_feedback = analysis['total_feedback']

        if low_ratings / total_feedback > 0.2:
            suggestions.append("ë‚®ì€ í‰ì (1-2ì )ì˜ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ë‹µë³€ ì •í™•ì„±ê³¼ ê´€ë ¨ì„±ì„ ê²€í† í•´ì£¼ì„¸ìš”.")

        if not suggestions:
            suggestions.append("í˜„ì¬ í”¼ë“œë°± ì ìˆ˜ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. ì§€ì†ì ì¸ í’ˆì§ˆ ìœ ì§€ì— ë…¸ë ¥í•´ì£¼ì„¸ìš”.")

        return suggestions

    def evaluate_response(self, question: str, answer: str, scores: Dict[str, int], 
                         comments: str = "", session_id: str = "", company: str = "") -> Dict[str, Any]:
        """ì‘ë‹µ í‰ê°€ ë° ì €ì¥ (Gradio ì¸í„°í˜ì´ìŠ¤ìš©)"""
        
        # ì ìˆ˜ ê²€ì¦
        for criterion, score in scores.items():
            if not (1 <= score <= 5):
                return {
                    "success": False,
                    "error": f"{criterion} ì ìˆ˜ëŠ” 1-5 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤."
                }
        
        # ì „ì²´ í‰ê·  ì ìˆ˜ ê³„ì‚°
        overall_score = sum(scores.values()) / len(scores)
        
        try:
            # ê¸°ì¡´ collect_feedback ë©”ì„œë“œ ì‚¬ìš©
            self.collect_feedback(
                query=question,
                company=company or "ì „ì²´",
                answer=answer,
                sources=[],
                rating=int(overall_score),
                feedback_text=comments
            )
            
            return {
                "success": True,
                "feedback_id": "saved",
                "overall_score": overall_score,
                "evaluation_summary": self._generate_evaluation_summary(scores, overall_score)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"í”¼ë“œë°± ì €ì¥ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _generate_evaluation_summary(self, scores: Dict[str, int], overall_score: float) -> str:
        """í‰ê°€ ìš”ì•½ ìƒì„±"""
        high_scores = [k for k, v in scores.items() if v >= 4]
        low_scores = [k for k, v in scores.items() if v <= 2]
        
        summary = f"ì „ì²´ í‰ì : {overall_score:.1f}/5.0\\n"
        
        if high_scores:
            summary += f"ê°•ì : {', '.join(high_scores)}\\n"
        
        if low_scores:
            summary += f"ê°œì„  í•„ìš”: {', '.join(low_scores)}\\n"
        
        return summary
    
    def get_feedback_stats(self) -> Dict[str, Any]:
        """í”¼ë“œë°± í†µê³„ ì¡°íšŒ (Gradio ì¸í„°í˜ì´ìŠ¤ìš©) - ì•ˆì „í•œ Dict key ì²˜ë¦¬"""
        try:
            analysis = self.analyze_feedback()
            if "error" in analysis:
                return analysis
            
            # Gradio í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ… - ëª¨ë“  keyë¥¼ ë¬¸ìì—´ë¡œ ë³´ì¥
            stats = {
                "ì´_í”¼ë“œë°±_ìˆ˜": analysis.get('total_feedback', 0),
                "í‰ê· _í‰ì ": f"{analysis.get('average_rating', 0):.2f}/5.0",
                "ë§Œì¡±ë„": f"{analysis.get('satisfaction_rate', 0):.1%}",
                "íšŒì‚¬ë³„_í‰ì ": self._safe_dict_keys(analysis.get('company_satisfaction', {})),
                "í‰ì _ë¶„í¬": self._safe_dict_keys(analysis.get('rating_distribution', {})),
                "ìµœê·¼_í”¼ë“œë°±": analysis.get('recent_feedback', [])
            }
            
            return stats
        except Exception as e:
            return {"ì˜¤ë¥˜": f"í”¼ë“œë°± í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}
    
    def _safe_dict_keys(self, d: Dict) -> Dict[str, Any]:
        """Dictì˜ ëª¨ë“  keyë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"""
        if not isinstance(d, dict):
            return {}
        return {str(k): v for k, v in d.items()}

    def show_feedback_analysis(self) -> str:
        """í”¼ë“œë°± ë¶„ì„ ê²°ê³¼ë¥¼ Gradioìš© ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
        analysis = self.analyze_feedback()
        suggestions = self.generate_improvement_suggestions()

        if "error" in analysis:
            return analysis["error"]

        result_text = "# ğŸ“ ì‚¬ìš©ì í”¼ë“œë°± ë¶„ì„\n\n"

        result_text += f"## ğŸ“Š ê¸°ë³¸ í†µê³„\n"
        result_text += f"- **ì´ í”¼ë“œë°± ìˆ˜**: {analysis['total_feedback']}\n"
        result_text += f"- **í‰ê·  í‰ì **: {analysis['average_rating']:.2f}/5\n"
        result_text += f"- **ë§Œì¡±ë„**: {analysis['satisfaction_rate']:.1%}\n\n"

        result_text += f"## ğŸ“ˆ í‰ì  ë¶„í¬\n"
        for rating, count in analysis['rating_distribution'].items():
            result_text += f"- **{rating}ì **: {count}íšŒ\n"

        if analysis.get('company_satisfaction'):
            result_text += f"\n## ğŸ¢ íšŒì‚¬ë³„ ë§Œì¡±ë„\n"
            for company, rating in analysis['company_satisfaction'].items():
                result_text += f"- **{company}**: {rating:.2f}/5\n"

        if suggestions:
            result_text += f"\n## ğŸ’¡ ê°œì„  ì œì•ˆ\n"
            for suggestion in suggestions:
                result_text += f"- {suggestion}\n"

        return result_text


class ABTestEvaluator:
    """A/B í…ŒìŠ¤íŠ¸ í‰ê°€ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.test_configs = {}
        self.test_results = {}

    def setup_ab_test(self, test_name: str, config_a: dict, config_b: dict):
        """A/B í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.test_configs[test_name] = {
            'config_a': config_a,
            'config_b': config_b,
            'results_a': [],
            'results_b': [],
            'start_time': datetime.now()
        }
        print(f"âœ… A/B í…ŒìŠ¤íŠ¸ '{test_name}' ì„¤ì • ì™„ë£Œ")

    def record_result(self, test_name: str, variant: str, metrics: dict):
        """ê²°ê³¼ ê¸°ë¡"""
        if test_name not in self.test_configs:
            print(f"âŒ í…ŒìŠ¤íŠ¸ '{test_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        if variant == 'A':
            self.test_configs[test_name]['results_a'].append(metrics)
        elif variant == 'B':
            self.test_configs[test_name]['results_b'].append(metrics)
        else:
            print(f"âŒ ì˜ëª»ëœ variant: {variant}")

    def analyze_ab_test(self, test_name: str) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ë¶„ì„"""
        if test_name not in self.test_configs:
            return {"error": f"í…ŒìŠ¤íŠ¸ '{test_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        test_data = self.test_configs[test_name]
        results_a = test_data['results_a']
        results_b = test_data['results_b']

        if not results_a or not results_b:
            return {"error": "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ
        analysis = {
            'test_name': test_name,
            'sample_size_a': len(results_a),
            'sample_size_b': len(results_b),
            'duration': (datetime.now() - test_data['start_time']).days
        }

        # ì‘ë‹µ ì‹œê°„ ë¹„êµ
        if all('response_time' in r for r in results_a + results_b):
            avg_time_a = statistics.mean([r['response_time'] for r in results_a])
            avg_time_b = statistics.mean([r['response_time'] for r in results_b])

            analysis['avg_response_time'] = {
                'variant_a': avg_time_a,
                'variant_b': avg_time_b,
                'improvement': ((avg_time_a - avg_time_b) / avg_time_a) * 100
            }

        # ì„±ê³µë¥  ë¹„êµ
        if all('success' in r for r in results_a + results_b):
            success_rate_a = sum(r['success'] for r in results_a) / len(results_a)
            success_rate_b = sum(r['success'] for r in results_b) / len(results_b)

            analysis['success_rate'] = {
                'variant_a': success_rate_a,
                'variant_b': success_rate_b,
                'improvement': ((success_rate_b - success_rate_a) / success_rate_a) * 100
            }

        return analysis


class RealTimeMonitor:
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.metrics = {
            'query_count': 0,
            'success_count': 0,
            'error_count': 0,
            'total_response_time': 0.0,
            'company_usage': defaultdict(int),
            'popular_queries': defaultdict(int),
            'recent_errors': [],
            'hourly_stats': defaultdict(lambda: {'queries': 0, 'errors': 0, 'response_time': 0.0})
        }
        self.lock = threading.Lock()
        self.start_time = datetime.now()

    def log_query(self, query: str, company: str, response_time: float,
                  success: bool, error_type: str = None):
        """ì¿¼ë¦¬ ë¡œê¹…"""
        with self.lock:
            current_hour = datetime.now().strftime('%Y-%m-%d %H:00')

            self.metrics['query_count'] += 1
            self.metrics['total_response_time'] += response_time
            self.metrics['company_usage'][company] += 1
            self.metrics['popular_queries'][query] += 1

            # ì‹œê°„ë³„ í†µê³„
            self.metrics['hourly_stats'][current_hour]['queries'] += 1
            self.metrics['hourly_stats'][current_hour]['response_time'] += response_time

            if success:
                self.metrics['success_count'] += 1
            else:
                self.metrics['error_count'] += 1
                self.metrics['hourly_stats'][current_hour]['errors'] += 1

                # ìµœê·¼ ì—ëŸ¬ ê¸°ë¡ (ìµœëŒ€ 10ê°œ)
                error_info = {
                    'timestamp': datetime.now().isoformat(),
                    'query': query,
                    'company': company,
                    'error_type': error_type or 'Unknown'
                }
                self.metrics['recent_errors'].append(error_info)
                if len(self.metrics['recent_errors']) > 10:
                    self.metrics['recent_errors'].pop(0)

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        with self.lock:
            if self.metrics['query_count'] == 0:
                return {"message": "ì•„ì§ ì¿¼ë¦¬ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

            avg_response_time = self.metrics['total_response_time'] / self.metrics['query_count']
            success_rate = (self.metrics['success_count'] / self.metrics['query_count']) * 100
            error_rate = (self.metrics['error_count'] / self.metrics['query_count']) * 100

            # ì¸ê¸° íšŒì‚¬ ë° ì¿¼ë¦¬
            top_companies = dict(Counter(self.metrics['company_usage']).most_common(3))
            top_queries = dict(Counter(self.metrics['popular_queries']).most_common(3))

            uptime = datetime.now() - self.start_time

            summary = {
                'total_queries': self.metrics['query_count'],
                'success_rate': f"{success_rate:.1f}%",
                'error_rate': f"{error_rate:.1f}%",
                'avg_response_time': f"{avg_response_time:.2f}ì´ˆ",
                'uptime': str(uptime).split('.')[0],  # ë§ˆì´í¬ë¡œì´ˆ ì œê±°
                'top_companies': top_companies,
                'top_queries': top_queries
            }

            return summary

    def get_alerts(self) -> List[str]:
        """ê²½ê³  ë©”ì‹œì§€ ë°˜í™˜"""
        alerts = []

        if self.metrics['query_count'] > 0:
            error_rate = (self.metrics['error_count'] / self.metrics['query_count']) * 100
            avg_response_time = self.metrics['total_response_time'] / self.metrics['query_count']

            if error_rate > 20:
                alerts.append(f"âš ï¸ ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.1f}% (ì„ê³„ê°’: 20%)")

            if avg_response_time > 10:
                alerts.append(f"âš ï¸ ëŠë¦° ì‘ë‹µ ì‹œê°„: {avg_response_time:.2f}ì´ˆ (ì„ê³„ê°’: 10ì´ˆ)")

            # ìµœê·¼ ì—ëŸ¬ í™•ì¸
            recent_errors = [e for e in self.metrics['recent_errors']
                           if (datetime.now() - datetime.fromisoformat(e['timestamp'])).seconds < 300]

            if len(recent_errors) >= 3:
                alerts.append("âš ï¸ ìµœê·¼ 5ë¶„ê°„ ì—ëŸ¬ê°€ 3íšŒ ì´ìƒ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        return alerts

    def reset_metrics(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        with self.lock:
            self.metrics = {
                'query_count': 0,
                'success_count': 0,
                'error_count': 0,
                'total_response_time': 0.0,
                'company_usage': defaultdict(int),
                'popular_queries': defaultdict(int),
                'recent_errors': [],
                'hourly_stats': defaultdict(lambda: {'queries': 0, 'errors': 0, 'response_time': 0.0})
            }
            self.start_time = datetime.now()
            print("âœ… ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


class GPTQualityEvaluator:
    """GPT ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""

    def __init__(self, llm):
        self.llm = llm
        self.test_queries = [
            "ì¹˜ë£Œë¹„ ë³´ì¥ í•œë„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
            "ë©´ì±…ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
            "ìˆ˜ìˆ ë¹„ëŠ” ì–´ë–»ê²Œ ë³´ì¥ë˜ë‚˜ìš”?",
            "ì˜ˆë°©ì ‘ì¢… ë¹„ìš©ì€ ë³´ì¥ë˜ë‚˜ìš”?",
            "ë³´í—˜ë£Œ ë‚©ì… ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ì²­êµ¬ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ë³´í—˜ ê°€ì… ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì¤‘ë³µë³´í—˜ ì²­êµ¬ê°€ ê°€ëŠ¥í•œê°€ìš”?",
            "ë³´í—˜ê¸ˆ ì§€ê¸‰ì´ ì œì™¸ë˜ëŠ” ê²½ìš°ëŠ”?",
            "ë°˜ë ¤ë™ë¬¼ ë‚˜ì´ ì œí•œì´ ìˆë‚˜ìš”?"
        ]

    def evaluate_quality(self, companies: List[str], rag_chain_func) -> str:
        """GPT ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰"""
        try:
            results_text = "# ğŸ”¬ GPT ê¸°ë°˜ ì‹œìŠ¤í…œ í‰ê°€ ê²°ê³¼\n\n"
            results_text += f"**í‰ê°€ ëŒ€ìƒ**: {', '.join(companies)}\n\n"

            for company in companies:
                try:
                    # ëœë¤ ì¿¼ë¦¬ ì„ íƒ
                    selected_query = random.choice(self.test_queries)

                    results_text += f"## ğŸ“‹ {company}\n"
                    results_text += f"**í…ŒìŠ¤íŠ¸ ì§ˆë¬¸**: {selected_query}\n\n"

                    # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    result = rag_chain_func(selected_query, [company])
                    
                    if isinstance(result, dict):
                        answer = result.get('answer', 'No answer')
                        sources = result.get('sources', [])
                    else:
                        answer = str(result)
                        sources = []

                    results_text += f"**RAG ì‹œìŠ¤í…œ ë‹µë³€**:\n{answer}\n\n"

                    if sources:
                        source_info = []
                        for src in sources[:2]:  # ìƒìœ„ 2ê°œë§Œ í‘œì‹œ
                            if isinstance(src, dict):
                                page_str = f"p.{src.get('page', 'n/a')}" if src.get('page') not in (None, "n/a") else "p.n/a"
                                source_info.append(f"{src.get('doc_id', 'unknown')} / {page_str}")
                            else:
                                source_info.append(str(src))
                        results_text += f"**ì¶œì²˜**: {', '.join(source_info)}\n\n"

                    # GPTë¥¼ í†µí•œ í’ˆì§ˆ í‰ê°€
                    evaluation_prompt = f"""
ë‹¤ìŒì€ í«ë³´í—˜ ê´€ë ¨ ì§ˆë¬¸ê³¼ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {selected_query}
ë‹µë³€: {answer}

ì´ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
1. ì •í™•ì„±: ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í–ˆëŠ”ê°€?
2. ì™„ì„±ë„: ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•œê°€?
3. ìœ ìš©ì„±: ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
4. ëª…í™•ì„±: ë‹µë³€ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?

í‰ê°€ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì ìˆ˜ë³´ë‹¤ëŠ” êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.
"""

                    # GPT í‰ê°€ ìš”ì²­
                    evaluation_response = self.llm.invoke(evaluation_prompt)
                    if hasattr(evaluation_response, 'content'):
                        gpt_evaluation = evaluation_response.content
                    else:
                        gpt_evaluation = str(evaluation_response)

                    results_text += f"**ğŸ¤– GPT í’ˆì§ˆ í‰ê°€**:\n{gpt_evaluation}\n\n"
                    results_text += "---\n\n"

                except Exception as e:
                    results_text += f"**âŒ {company} í‰ê°€ ì‹¤íŒ¨**: {str(e)}\n\n"
                    continue

            results_text += "### ğŸ“ˆ ì¢…í•© í‰ê°€ ì™„ë£Œ\n"
            results_text += "ê° íšŒì‚¬ë³„ë¡œ ëœë¤ ì„ íƒëœ ì§ˆë¬¸ì— ëŒ€í•œ GPT ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            results_text += "í‰ê°€ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì‹œìŠ¤í…œ ê°œì„ ì— í™œìš©í•˜ì„¸ìš”.\n"

            return results_text

        except Exception as e:
            return f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def create_evaluation_system(db_path: str = None):
    """í‰ê°€ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    if db_path is None:
        db_path = Path(__file__).parent.parent / "user_feedback.db"
    
    user_feedback_evaluator = UserFeedbackEvaluator(str(db_path))
    ab_test_evaluator = ABTestEvaluator()
    real_time_monitor = RealTimeMonitor()
    
    return user_feedback_evaluator, ab_test_evaluator, real_time_monitor


if __name__ == "__main__":
    print("âœ… í‰ê°€ ì‹œìŠ¤í…œ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!")
    print("  - UserFeedbackEvaluator: í”¼ë“œë°± ìˆ˜ì§‘ ë° ë¶„ì„")
    print("  - ABTestEvaluator: A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬")
    print("  - RealTimeMonitor: ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print("  - GPTQualityEvaluator: GPT ê¸°ë°˜ í’ˆì§ˆ í‰ê°€")